# Fly.io configuration for Secondary Inference Workers
# Deploy Python workers for processing primary inference outputs

app = "secondary-inference-workers"
primary_region = "sjc"  # San Jose, California

[build]

[env]
  # Supabase configuration (will be set via fly secrets)
  SUPABASE_URL = ""
  SUPABASE_SERVICE_ROLE_KEY = ""
  DATABASE_URL = ""
  
  # Storage configuration
  SUPABASE_STORAGE_BUCKET = "secondary-inference"
  
  # Worker configuration
  WORKER_TYPE = "inference"
  MAX_CONCURRENT_JOBS = "10"
  LOG_LEVEL = "INFO"

[http_service]
  internal_port = 8000
  force_https = true
  auto_stop_machines = true
  auto_start_machines = true
  min_machines_running = 1
  processes = ["app"]

  [[http_service.checks]]
    grace_period = "10s"
    interval = "30s"
    method = "GET"
    timeout = "5s"
    path = "/health"

[[vm]]
  cpu_kind = "shared"
  cpus = 2
  memory_mb = 4096

  # GPU configuration for ML workloads
  [[vm.gpu]]
    kind = "A10"
    count = 1

[deploy]
  release_command = "python -m pip install -e ."

# Scaling configuration
[[services]]
  protocol = "tcp"
  internal_port = 8000

  [[services.ports]]
    port = 80
    handlers = ["http"]
    force_https = true

  [[services.ports]]
    port = 443
    handlers = ["tls", "http"]

  [services.concurrency]
    type = "connections"
    hard_limit = 1000
    soft_limit = 800

# Health checks
[[services.tcp_checks]]
  interval = "15s"
  timeout = "2s"
  grace_period = "1s"

# Metrics and monitoring
[metrics]
  port = 9090
  path = "/metrics"
